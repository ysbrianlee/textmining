{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감성 분석\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 감성 사전을 이용한 영화 리뷰 감성 분석\n",
    "\n",
    "### NLTK 영화 리뷰 데이터 준비\n",
    "\n",
    " https://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\ysbri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#review count: 2000\n",
      "#samples of file ids: ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
      "#categories of reviews: ['neg', 'pos']\n",
      "#num of \"neg\" reviews: 1000\n",
      "#num of \"pos\" reviews: 1000\n",
      "#id of the first review: neg/cv000_29416.txt\n",
      "#part of the first review: plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt\n",
      "#sentiment of the first review: ['neg']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "print('#review count:', len(movie_reviews.fileids())) #영화 리뷰 문서의 id를 반환\n",
    "print('#samples of file ids:', movie_reviews.fileids()[:10]) #id를 10개까지만 출력\n",
    "print('#categories of reviews:', movie_reviews.categories()) # label, 즉 긍정인지 부정인지에 대한 분류\n",
    "print('#num of \"neg\" reviews:', len(movie_reviews.fileids(categories='neg'))) #label이 부정인 문서들의 id를 반환\n",
    "print('#num of \"pos\" reviews:', len(movie_reviews.fileids(categories='pos'))) #label이 긍정인 문서들의 id를 반환\n",
    "\n",
    "fileid = movie_reviews.fileids()[0] #첫번째 문서의 id를 반환\n",
    "print('#id of the first review:', fileid)\n",
    "print('#part of the first review:', movie_reviews.raw(fileid)[:500]) #첫번째 문서의 내용을 500자까지만 출력\n",
    "print('#sentiment of the first review:', movie_reviews.categories(fileid)) #첫번째 문서의 감성\n",
    "\n",
    "fileids = movie_reviews.fileids() #movie review data에서 file id를 가져옴\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids] #file id를 이용해 raw text file을 가져옴\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob을 이용한 감성 분석\n",
    "\n",
    " https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://textblob.readthedocs.io/en/dev/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\ysbri\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\ysbri\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\ysbri\\appdata\\roaming\\python\\python39\\site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\ysbri\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ysbri\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ysbri\\appdata\\roaming\\python\\python39\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ysbri\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\ysbri\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\ysbri\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ysbri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ysbri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ysbri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ysbri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\ysbri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\ysbri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.06479782948532947, subjectivity=0.5188408350908352)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "result = TextBlob(reviews[0])\n",
    "print(result.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_TextBlob(docs):\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        testimonial = TextBlob(doc)\n",
    "        if testimonial.sentiment.polarity > 0:\n",
    "            results.append('pos')\n",
    "        else:\n",
    "            results.append('neg')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#TextBlob을 이용한 리뷰 감성분석의 정확도: 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('#TextBlob을 이용한 리뷰 감성분석의 정확도:', accuracy_score(categories, sentiment_TextBlob(reviews)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFINN을 이용한 감성 분석\n",
    "\n",
    "https://github.com/fnielsen/afinn \n",
    "\n",
    " http://corpustext.com/reference/sentiment_afinn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting afinn\n",
      "  Downloading afinn-0.1.tar.gz (52 kB)\n",
      "     ---------------------------------------- 52.6/52.6 kB 2.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: afinn\n",
      "  Building wheel for afinn (setup.py): started\n",
      "  Building wheel for afinn (setup.py): finished with status 'done'\n",
      "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53447 sha256=c7d4285f977acdf1a775d02604a50e4df74c90dc70a51a7193b84d0207f7584d\n",
      "  Stored in directory: c:\\users\\ysbri\\appdata\\local\\pip\\cache\\wheels\\79\\91\\ee\\8374d9bc8c6c0896a2db75afdfd63d43653902407a0e76cd94\n",
      "Successfully built afinn\n",
      "Installing collected packages: afinn\n",
      "Successfully installed afinn-0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\ysbri\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\ysbri\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Afinn을 이용한 리뷰 감성분석의 정확도: 0.664\n"
     ]
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "def sentiment_Afinn(docs):\n",
    "    afn = Afinn(emoticons=True)\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        if afn.score(doc) > 0:\n",
    "            results.append('pos')\n",
    "        else:\n",
    "            results.append('neg')\n",
    "    return results\n",
    "\n",
    "print('#Afinn을 이용한 리뷰 감성분석의 정확도:', accuracy_score(categories, sentiment_Afinn(reviews)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER를 이용한 감성 분석\n",
    "\n",
    "https://github.com/cjhutto/vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ysbri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Vader을 이용한 리뷰 감성분석의 정확도: 0.635\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def sentiment_vader(docs):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        score = analyser.polarity_scores(doc)\n",
    "        if score['compound'] > 0:\n",
    "            results.append('pos')\n",
    "        else:\n",
    "            results.append('neg')\n",
    "\n",
    "    return results\n",
    "\n",
    "print('#Vader을 이용한 리뷰 감성분석의 정확도:', accuracy_score(categories, sentiment_vader(reviews)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 감성사전\n",
    "\n",
    " https://github.com/park1200656/KnuSentiLex   \n",
    " https://github.com/mrlee23/KoreanSentimentAnalyzer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습을 통한 머신러닝 기반의 감성 분석\n",
    "\n",
    "### NLTK 영화 리뷰에 대한 머신러닝 기반 감성 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count:  1600\n",
      "Test set count:  400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split #sklearn에서 제공하는 split 함수를 사용\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, categories, test_size=0.2, random_state=7)\n",
    "\n",
    "print('Train set count: ', len(X_train))\n",
    "print('Test set count: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set dimension: (1600, 36189)\n",
      "#Test set dimension: (400, 36189)\n",
      "#Train set score: 0.998\n",
      "#Test set score: 0.797\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB #sklearn이 제공하는 MultinomialNB 를 사용\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(X_train) \n",
    "\n",
    "X_train_tfidf = tfidf.transform(X_train) # train set을 변환\n",
    "print('#Train set dimension:', X_train_tfidf.shape) # 실제로 몇개의 특성이 사용되었는지 확인\n",
    "X_test_tfidf = tfidf.transform(X_test) # test set을 변환\n",
    "print('#Test set dimension:', X_test_tfidf.shape)\n",
    "\n",
    "NB_clf = MultinomialNB(alpha=0.01) # 분류기 선언\n",
    "NB_clf.fit(X_train_tfidf, y_train) #train set을 이용하여 분류기(classifier)를 학습\n",
    "print('#Train set score: {:.3f}'.format(NB_clf.score(X_train_tfidf, y_train))) #train set에 대한 예측정확도를 확인\n",
    "print('#Test set score: {:.3f}'.format(NB_clf.score(X_test_tfidf, y_test))) #test set에 대한 예측정확도를 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 영화 리뷰에 대한 머신러닝 기반 감성 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재미있다</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date   \n",
       "0                             돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29  \\\n",
       "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n",
       "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n",
       "3                                이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n",
       "4                                               재미있다      10  2018.10.20   \n",
       "\n",
       "    title  \n",
       "0  인피니티 워  \n",
       "1  인피니티 워  \n",
       "2  인피니티 워  \n",
       "3  인피니티 워  \n",
       "4  인피니티 워  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEJCAYAAAB4yveGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAU5klEQVR4nO3df7Bc5X3f8fcHYWNswAZzURUJKtIqToDG2CgyKZ3YLq6RQxrRH3TkTIPq4mpKcW3PpNOItDOZ/KEOns6kCTOBGcXYiIltRnbjQbWDA1GMO2mJ4WIwQgiCAlhoBJLs+AcmGVzkb//YR9OtWOmuxN0V0vN+zezs2e+eZ7/nXN372aNnz+6mqpAk9eGkY70BkqTpMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpy8rHegLmcffbZtXTp0mO9GZJ0XHnwwQe/XVUzB9df86G/dOlSZmdnj/VmSNJxJcm3RtWd3pGkjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR15DX/5ixJOlEtXfflox77zI1XHtU4j/QlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR8YK/SRvSfKFJI8n2Z7k55OcleSeJE+26zOH1r8hyY4kTyS5Yqh+SZKt7b6bkmQSOyVJGm3cI/3fBb5SVT8NvB3YDqwDtlTVMmBLu02SC4DVwIXASuDmJAva49wCrAWWtcvKedoPSdIY5gz9JGcAvwDcClBVP6qq7wGrgI1ttY3AVW15FXBHVb1UVU8DO4AVSRYBZ1TVfVVVwO1DYyRJUzDOkf5PAvuATyd5KMknk7wJWFhVzwG063Pa+ouBZ4fG72q1xW354LokaUrGCf2TgXcCt1TVO4AXaVM5hzBqnr4OU3/lAyRrk8wmmd23b98YmyhJGsc4ob8L2FVVX2+3v8DgSWBPm7KhXe8dWv/cofFLgN2tvmRE/RWqakNVLa+q5TMzr/gyd0nSUZoz9KvqeeDZJG9rpcuBx4DNwJpWWwPc2ZY3A6uTnJLkfAYv2N7fpoBeSHJpO2vnmqExkqQpGPdTNv898JkkrweeAj7E4AljU5JrgZ3A1QBVtS3JJgZPDC8D11fV/vY41wG3AacCd7WLJGlKxgr9qnoYWD7irssPsf56YP2I+ixw0RFsnyRpHvmOXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkfGCv0kzyTZmuThJLOtdlaSe5I82a7PHFr/hiQ7kjyR5Iqh+iXtcXYkuSlJ5n+XJEmHciRH+u+tqouranm7vQ7YUlXLgC3tNkkuAFYDFwIrgZuTLGhjbgHWAsvaZeWr3wVJ0rhezfTOKmBjW94IXDVUv6OqXqqqp4EdwIoki4Azquq+qirg9qExkqQpGDf0C7g7yYNJ1rbawqp6DqBdn9Pqi4Fnh8buarXFbfnguiRpSk4ec73Lqmp3knOAe5I8fph1R83T12Hqr3yAwRPLWoDzzjtvzE2UJM1lrCP9qtrdrvcCXwRWAHvalA3tem9bfRdw7tDwJcDuVl8yoj6q34aqWl5Vy2dmZsbfG0nSYc0Z+knelOT0A8vA+4FHgc3AmrbaGuDOtrwZWJ3klCTnM3jB9v42BfRCkkvbWTvXDI2RJE3BONM7C4EvtrMrTwY+W1VfSfIAsCnJtcBO4GqAqtqWZBPwGPAycH1V7W+PdR1wG3AqcFe7SJKmZM7Qr6qngLePqH8HuPwQY9YD60fUZ4GLjnwzJUnzwXfkSlJHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOjJ26CdZkOShJF9qt89Kck+SJ9v1mUPr3pBkR5InklwxVL8kydZ2301JMr+7I0k6nCM50v8YsH3o9jpgS1UtA7a02yS5AFgNXAisBG5OsqCNuQVYCyxrl5WvauslSUdkrNBPsgS4EvjkUHkVsLEtbwSuGqrfUVUvVdXTwA5gRZJFwBlVdV9VFXD70BhJ0hSMe6T/O8B/BH48VFtYVc8BtOtzWn0x8OzQertabXFbPrj+CknWJplNMrtv374xN1GSNJc5Qz/JLwF7q+rBMR9z1Dx9Hab+ymLVhqpaXlXLZ2ZmxmwrSZrLyWOscxnwy0l+EXgDcEaSPwD2JFlUVc+1qZu9bf1dwLlD45cAu1t9yYi6JGlK5jzSr6obqmpJVS1l8ALtn1bVvwQ2A2vaamuAO9vyZmB1klOSnM/gBdv72xTQC0kubWftXDM0RpI0BeMc6R/KjcCmJNcCO4GrAapqW5JNwGPAy8D1VbW/jbkOuA04FbirXSRJU3JEoV9V9wL3tuXvAJcfYr31wPoR9VngoiPdSEnS/PAduZLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI7MGfpJ3pDk/iTfTLItyW+1+llJ7knyZLs+c2jMDUl2JHkiyRVD9UuSbG333ZQkk9ktSdIo4xzpvwT8w6p6O3AxsDLJpcA6YEtVLQO2tNskuQBYDVwIrARuTrKgPdYtwFpgWbusnL9dkSTNZc7Qr4Eftpuva5cCVgEbW30jcFVbXgXcUVUvVdXTwA5gRZJFwBlVdV9VFXD70BhJ0hSMNaefZEGSh4G9wD1V9XVgYVU9B9Cuz2mrLwaeHRq+q9UWt+WD66P6rU0ym2R23759R7A7kqTDGSv0q2p/VV0MLGFw1H7RYVYfNU9fh6mP6rehqpZX1fKZmZlxNlGSNIYjOnunqr4H3MtgLn5Pm7KhXe9tq+0Czh0atgTY3epLRtQlSVMyztk7M0ne0pZPBd4HPA5sBta01dYAd7blzcDqJKckOZ/BC7b3tymgF5Jc2s7auWZojCRpCk4eY51FwMZ2Bs5JwKaq+lKS+4BNSa4FdgJXA1TVtiSbgMeAl4Hrq2p/e6zrgNuAU4G72kWSNCVzhn5VPQK8Y0T9O8DlhxizHlg/oj4LHO71AEnSBPmOXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6MmfoJzk3yVeTbE+yLcnHWv2sJPckebJdnzk05oYkO5I8keSKofolSba2+25KksnsliRplHGO9F8Gfq2qfga4FLg+yQXAOmBLVS0DtrTbtPtWAxcCK4Gbkyxoj3ULsBZY1i4r53FfJElzOHmuFarqOeC5tvxCku3AYmAV8J622kbgXuDXW/2OqnoJeDrJDmBFkmeAM6rqPoAktwNXAXfN3+5I0pFZuu7LRz32mRuvnMctmY4jmtNPshR4B/B1YGF7QjjwxHBOW20x8OzQsF2ttrgtH1yXJE3J2KGf5DTgvwMfr6ofHG7VEbU6TH1Ur7VJZpPM7tu3b9xNlCTNYazQT/I6BoH/mar6w1bek2RRu38RsLfVdwHnDg1fAuxu9SUj6q9QVRuqanlVLZ+ZmRl3XyRJcxjn7J0AtwLbq+q3h+7aDKxpy2uAO4fqq5OckuR8Bi/Y3t+mgF5Icml7zGuGxkiSpmDOF3KBy4BfBbYmebjVfgO4EdiU5FpgJ3A1QFVtS7IJeIzBmT/XV9X+Nu464DbgVAYv4PoiriRN0Thn7/wZo+fjAS4/xJj1wPoR9VngoiPZQEnS/PEduZLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOjLOd+RK0sQtXfflox77zI1XzuOWnNg80pekjhj6ktQRQ1+SOuKc/lFy/lHS8cgjfUnqyJyhn+RTSfYmeXSodlaSe5I82a7PHLrvhiQ7kjyR5Iqh+iVJtrb7bkqS+d8dSdLhjHOkfxuw8qDaOmBLVS0DtrTbJLkAWA1c2MbcnGRBG3MLsBZY1i4HP6YkacLmDP2q+p/AXx1UXgVsbMsbgauG6ndU1UtV9TSwA1iRZBFwRlXdV1UF3D40RpI0JUc7p7+wqp4DaNfntPpi4Nmh9Xa12uK2fHB9pCRrk8wmmd23b99RbqIk6WDz/ULuqHn6Okx9pKraUFXLq2r5zMzMvG2cJPXuaEN/T5uyoV3vbfVdwLlD6y0Bdrf6khF1SdIUHW3obwbWtOU1wJ1D9dVJTklyPoMXbO9vU0AvJLm0nbVzzdAYSdKUzPnmrCSfA94DnJ1kF/CbwI3ApiTXAjuBqwGqaluSTcBjwMvA9VW1vz3UdQzOBDoVuKtdJL2GvJo3HYJvPDwezBn6VfXBQ9x1+SHWXw+sH1GfBS46oq2TJM0r35ErSR0x9CWpI4a+JHXE0Jekjhj6ktQRP09fOgRPX9SJ6LgOff8oJenIHNehL52o/GY2TYpz+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTz9DU2zx2Xjn+G/nHG4JX0ahj6es3ziU6aP87pS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI5MPfSTrEzyRJIdSdZNu78k9WyqoZ9kAfB7wAeAC4APJrlgmtsgST2b9pH+CmBHVT1VVT8C7gBWTXkbJKlbqarpNUv+ObCyqj7cbv8q8K6q+shB660F1rabbwOeOMqWZwPfPsqxr8ax6nsse7vPffTure+x7P1q+/7tqpo5uDjtj2HIiNornnWqagOw4VU3S2aravmrfZzjpe+x7O0+99G7t77Hsvek+k57emcXcO7Q7SXA7ilvgyR1a9qh/wCwLMn5SV4PrAY2T3kbJKlbU53eqaqXk3wE+GNgAfCpqto2wZaveoroOOt7LHu7z3307q3vsew9kb5TfSFXknRs+Y5cSeqIoS9JHTH0JakjJ9TXJSb5aQbv8F3M4Pz/3cDmqtp+TDdsgto+Lwa+XlU/HKqvrKqvTLDvCqCq6oH2URorgcer6o8m1fMQ23F7VV0zzZ6t7z9g8A7zR6vq7gn2eRewvap+kORUYB3wTuAx4L9U1fcn2PujwBer6tlJ9ThE3wNn9u2uqj9J8ivA3we2Axuq6v9MsPffAf4Jg1PLXwaeBD43yZ/ztJ0wL+Qm+XXggww+2mFXKy9h8MtzR1XdeAy26UNV9ekJPv5HgesZ/DFcDHysqu5s932jqt45ob6/yeDzk04G7gHeBdwLvA/446paP6G+B5/eG+C9wJ8CVNUvT6Jv631/Va1oy/+Gwc/9i8D7gf8xqd+vJNuAt7cz3zYAfw18Abi81f/pJPq23t8HXgT+Evgc8Pmq2jepfkN9P8Pgd+uNwPeA04A/ZLDPqao1E+r7UeAfA18DfhF4GPgugyeBf1dV906i79RV1QlxAf4CeN2I+uuBJ4/RNu2c8ONvBU5ry0uBWQbBD/DQhPsuYPBH+QPgjFY/FXhkgn2/AfwB8B7g3e36ubb87gn/rB8aWn4AmGnLbwK2TrDv9uH9P+i+hye9zwymgN8P3ArsA74CrAFOn2DfR9r1ycAeYEG7nQn/fm0d6vVG4N62fN4k/55ajzcDNwKPA99pl+2t9pb57HUizen/GPiJEfVF7b6JSPLIIS5bgYWT6tssqDalU1XPMAjBDyT5bUZ/5MV8ebmq9lfVXwN/WVU/aNvwN0zwZw0sBx4E/hPw/Rocef1NVX2tqr42wb4AJyU5M8lbGRxt7gOoqhcZTANMyqNJPtSWv5lkOUCSnwImNs3RVFX9uKrurqprGfx93cxgKu+pCfY9qU3xnM4gfN/c6qcAr5tgX/h/U96ntP5U1c4p9N3E4H8V76mqt1bVWxn8L/a7wOfns9GJNKf/cWBLkieBA3OQ5wF/F/jIoQbNg4XAFQz+cYYF+N8T7AvwfJKLq+phgKr6YZJfAj4F/L0J9v1Rkje20L/kQDHJm5lg6FfVj4H/luTz7XoP0/sdfjODJ5wAleRvVdXzSU5jsk+wHwZ+N8l/ZvDhW/cleZbB7/iHJ9gXDtqvGsylbwY2t9cXJuVWBke8Cxg8wX8+yVPApQymbyflk8ADSf4c+AXgEwBJZoC/mmBfgKVV9YnhQlU9D3wiyb+ez0YnzJw+QJKTGLy4tpjBL+wu4IGq2j/BnrcCn66qPxtx32er6lcm2HsJg6Pu50fcd1lV/a8J9T2lql4aUT8bWFRVWyfRd0S/K4HLquo3ptHvENvwRmBhVT094T6nAz/J4EluV1XtmWS/1vOnquovJt3nEL1/AqCqdid5C4PXi3ZW1f0T7nsh8DMMXqB/fJK9Dup7N/AnwMYD/7ZJFgL/CvhHVfW+eet1IoW+JB2PkpzJ4MysVcA5rbyHwf+sbqyqg2cSjr6XoS9Jr13zfRagoS9Jr2FJdlbVefP1eCfSC7mSdFxK8sih7mKezwI09CXp2JvaWYCGviQde19i8EbLhw++I8m989nIOX1J6siJ9I5cSdIcDH1J6oihL40pycfbO3AP3P6j9m5R6bjhnL40JEkY/F284jOEkjwDLK+qb099w6R54pG+updkaZLtSW5m8PHNtyaZTbItyW+1dT7K4FMmv5rkq632TJKzh8b/fhtz94EPJEvyc+1TV+9L8l+TPHqs9lMCQ1864G3A7VX1DuDXqmo58LPAu5P8bFXdxOCb2N5bVe8dMX4Z8HtVdSGDL/74Z63+aeDfVtXPAxP74D9pXIa+NPCtqvrztvwvknyDwZeIXAhcMMb4p4fOsX4QWNrm+0+vqgNvrvnsPG6vdFR8c5Y08CJAkvOB/wD8XFV9N8ltwBvGGD/8UdP7GXyL2CQ/Z186Kh7pS/+/Mxg8AXy/fZ75B4bue4H2bUrjaB+H+0KSS1tp9bxtpXSUPNKXhlTVN5M8BGxj8JWAw19EswG4K8lzh5jXH+Va4PeTvMjgy+O/P5/bKx0pT9mUJijJaQe+xzjJOgbfLPaxY7xZ6phH+tJkXZnkBgZ/a99i8PV30jHjkb4kdcQXciWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JH/i8iQO0MX86b+AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df.rating.value_counts().sort_index().plot(kind='bar')\n",
    "#df.rating.plot.hist(bins=10) #히스토그램을 그릴 수도 있다.\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| | 긍정으로 예측한 리뷰(PP) | 부정으로 예측한 리뷰(PN) |\n",
    "|---|---|---|\n",
    "|실제 긍정인 리뷰(P) | True positive(TP) | False negative(FN) |\n",
    "|실제 부정인 리뷰(N) | False positive(FP) | True negative(TN) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train set size: 11043\n",
      "#Test set size: 3682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data and labels into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.review, df.rating, random_state=7)\n",
    "print('#Train set size:', len(X_train))\n",
    "print('#Test set size:', len(X_test))\n",
    "\n",
    "from konlpy.tag import Okt #konlpy에서 Twitter 형태소 분석기를 import\n",
    "#from konlpy.tag import Twitter #konlpy에서 Twitter 형태소 분석기를 import\n",
    "okt = Okt()\n",
    "\n",
    "def twit_tokenizer(text): #전체를 다 사용하는 대신, 명사, 동사, 형용사를 사용\n",
    "    target_tags = ['Noun', 'Verb', 'Adjective']\n",
    "    result = []\n",
    "    for word, tag in okt.pos(text, norm=True, stem=True):\n",
    "        if tag in target_tags:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, max_features=2000, min_df=5, max_df=0.5) #명사, 동사, 형용사를 이용하여 tfidf 생성\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Regression Train set R2 score: 0.605\n",
      "#Regression Test set R2 score: 0.395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()  #객체를 생성\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "print('#Regression Train set R2 score: {:.3f}'.format(lr.score(X_train_tfidf, y_train)))\n",
    "print('#Regression Test set R2 score: {:.3f}'.format(lr.score(X_test_tfidf, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Accuracy for train set: 0.888\n",
      "#Precision for train set: 0.893\n",
      "#Recall for train set: 0.969\n",
      "#F1 for train set: 0.929\n",
      "#Accuracy for test set: 0.848\n",
      "#Precision for test set: 0.868\n",
      "#Recall for test set: 0.946\n",
      "#F1 for test set: 0.905\n"
     ]
    }
   ],
   "source": [
    "y_train_senti = (y_train > 5)\n",
    "y_test_senti = (y_test > 5)\n",
    "\n",
    "y_train_predict = (lr.predict(X_train_tfidf) > 5)\n",
    "y_test_predict = (lr.predict(X_test_tfidf) > 5)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('#Accuracy for train set: {:.3f}'.format(accuracy_score(y_train_senti, y_train_predict)))\n",
    "print('#Precision for train set: {:.3f}'.format(precision_score(y_train_senti, y_train_predict)))\n",
    "print('#Recall for train set: {:.3f}'.format(recall_score(y_train_senti, y_train_predict)))\n",
    "print('#F1 for train set: {:.3f}'.format(f1_score(y_train_senti, y_train_predict)))\n",
    "\n",
    "print('#Accuracy for test set: {:.3f}'.format(accuracy_score(y_test_senti, y_test_predict)))\n",
    "print('#Precision for test set: {:.3f}'.format(precision_score(y_test_senti, y_test_predict)))\n",
    "print('#Recall for test set: {:.3f}'.format(recall_score(y_test_senti, y_test_predict)))\n",
    "print('#F1 for test set: {:.3f}'.format(f1_score(y_test_senti, y_test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Accuracy for train set: 0.878\n",
      "#Precision for train set: 0.878\n",
      "#Recall for train set: 0.973\n",
      "#F1 for train set: 0.923\n",
      "#Accuracy for test set: 0.855\n",
      "#Precision for test set: 0.866\n",
      "#Recall for test set: 0.958\n",
      "#F1 for test set: 0.910\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression #sklearn이 제공하는 logistic regression을 사용\n",
    "\n",
    "#count vector에 대해 regression을 해서 NB와 비교\n",
    "LR_clf = LogisticRegression() #분류기 선언\n",
    "LR_clf.fit(X_train_tfidf, y_train_senti) # train data를 이용하여 분류기를 학습\n",
    "\n",
    "y_train_predict = LR_clf.predict(X_train_tfidf)\n",
    "y_test_predict = LR_clf.predict(X_test_tfidf)\n",
    "\n",
    "print('#Accuracy for train set: {:.3f}'.format(accuracy_score(y_train_senti, y_train_predict)))\n",
    "print('#Precision for train set: {:.3f}'.format(precision_score(y_train_senti, y_train_predict)))\n",
    "print('#Recall for train set: {:.3f}'.format(recall_score(y_train_senti, y_train_predict)))\n",
    "print('#F1 for train set: {:.3f}'.format(f1_score(y_train_senti, y_train_predict)))\n",
    "\n",
    "print('#Accuracy for test set: {:.3f}'.format(accuracy_score(y_test_senti, y_test_predict)))\n",
    "print('#Precision for test set: {:.3f}'.format(precision_score(y_test_senti, y_test_predict)))\n",
    "print('#Recall for test set: {:.3f}'.format(recall_score(y_test_senti, y_test_predict)))\n",
    "print('#F1 for test set: {:.3f}'.format(f1_score(y_test_senti, y_test_predict)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "\n",
    "Finn Årup Nielsen A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings 93-98. 2011 May.\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
